import json
import requests
import re
from pathlib import Path
from datetime import datetime

# è¯»å–é…ç½®
with open("config.json", "r", encoding="utf-8") as f:
    config = json.load(f)

output_lines = []
invalid_links = []

def fetch_content(url, ua=None):
    headers = {}
    if ua:
        headers["User-Agent"] = ua
    r = requests.get(url, headers=headers, timeout=15)
    r.raise_for_status()
    return r.text

def check_url(url):
    try:
        r = requests.head(url, timeout=10, allow_redirects=True)
        return r.status_code == 200
    except Exception:
        return False

# å¤„ç†æ¯ä¸ªæº
for source in config["sources"]:
    ua_default = source.get("UA", [None])[0]  # é»˜è®¤ UA
    content = fetch_content(source["url"], ua=ua_default)

    lines = content.splitlines()

    group_filters = set(source.get("groups", []))
    channel_filters = set(source.get("channels", []))
    keyword_filters = source.get("keywords", [])

    keep_channel = False
    channel_lines = []

    for line in lines:
        if line.startswith("#EXTINF"):
            # å¦‚æœä¹‹å‰æœ‰ç¼“å­˜çš„é¢‘é“å¹¶ä¸”éœ€è¦ä¿ç•™ â†’ å†™å…¥
            if keep_channel and channel_lines:
                output_lines.extend(channel_lines)

            # å¼€å§‹æ–°çš„é¢‘é“æ®µ
            channel_lines = [line]
            keep_channel = False

            # æå–ç»„åå’Œé¢‘é“å
            group_match = re.search(r'group-title="([^"]+)"', line)
            name_match = re.search(r',(.+)', line)
            group = group_match.group(1) if group_match else ""
            name = name_match.group(1).strip() if name_match else ""

            # åˆ¤æ–­æ˜¯å¦ä¿ç•™è¯¥é¢‘é“
            if group in group_filters or name in channel_filters or any(k in name for k in keyword_filters):
                keep_channel = True

        else:
            # æ™®é€šè¡Œï¼ˆå¯èƒ½æ˜¯ URL æˆ–å…¶ä»–ï¼‰
            if line.startswith("http") and keep_channel:
                url = line.strip()
                if config.get("check_urls", False):
                    if not check_url(url):
                        invalid_links.append(f"{name}: {url}")
                        continue
                if "|User-Agent=" not in url and ua_default:
                    url = f"{url}|User-Agent={ua_default}"
                channel_lines.append(url)
            elif keep_channel:
                channel_lines.append(line)

    # æœ€åä¸€ä¸ªé¢‘é“å¦‚æœéœ€è¦ä¿ç•™ â†’ å†™å…¥
    if keep_channel and channel_lines:
        output_lines.extend(channel_lines)

# åœ¨æœ€å‰é¢åŠ ä¸Š M3U å¤´
output_lines.insert(0, "#EXTM3U")

# å†™å…¥è¾“å‡ºæ–‡ä»¶
output_path = Path(config["output"])
output_path.write_text("\n".join(output_lines), encoding="utf-8")

# ç”Ÿæˆæ—¥å¿—
log_path = Path("output.log")
with log_path.open("w", encoding="utf-8") as log:
    log.write(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
    log.write(f"å…±è¾“å‡ºé¢‘é“: {output_lines.count('#EXTINF')}\n")
    log.write(f"æ— æ•ˆé“¾æ¥: {len(invalid_links)}\n\n")
    if invalid_links:
        log.write("ä»¥ä¸‹é“¾æ¥æ£€æµ‹å¤±è´¥:\n")
        for item in invalid_links:
            log.write(f"{item}\n")

print(f"âœ… ç”Ÿæˆå®Œæˆ: {output_path}ï¼Œå…± {output_lines.count('#EXTINF')} ä¸ªé¢‘é“")
print(f"ğŸ“„ æ—¥å¿—: {log_path}ï¼Œè®°å½•äº† {len(invalid_links)} ä¸ªæ— æ•ˆé“¾æ¥")
